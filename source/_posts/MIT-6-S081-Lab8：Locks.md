---
title: 'MIT-6-S081-Lab8：Locks'
date: 2025-04-10 16:19:44
categories: 6.s081
tags: [操作系统, 自旋锁, 睡眠锁, Linux, 6.s081]
---
# 知识点
2000年左右，随着技术的进步，CPU 的时钟频率达到了一个极限，单线程性能也随之趋于瓶颈。然而，随着晶体管数量的不断增加，CPU 开始向多核架构发展。因此，大多数现代操作系统采用多核处理器，交替执行多个任务。虽然这种并行执行大大提高了系统的总体处理能力，但也带来了并发性问题和潜在的风险。

在并发环境中，多个进程或线程可能会访问共享资源，这就需要一种机制来确保数据的一致性和任务的正确性。这时，**锁**成为了至关重要的工具。锁通过同步并发任务的执行，确保在任何时刻只有一个进程或线程能访问某个共享资源，从而避免竞态条件、数据竞争等问题，确保程序在并发环境中的正确性和有序执行。

锁的设计和实现，随着多核处理器的普及，成为了操作系统中不可或缺的一部分。通过精确的锁机制，操作系统能够有效地管理多个并发执行的活动，保证系统稳定、可靠地运行。

## 并发控制与锁的原理
在讨论设备中断时，我们已经意识到并发带来的风险。接下来，我们将深入探讨如何有效应对这些风险。

操作系统能够交错执行多个活动，部分原因在于它拥有多个处理器硬件，这些 CPU 独立执行任务，但共享物理内存。XV6 利用这种共享来维护跨 CPU 的数据结构，如进程表或调度器状态。为了保证这些数据结构在并发访问下的一致性和正确性，系统需要采取措施避免多个 CPU 同时访问或修改相同的数据，进而导致数据损坏。此外，即使在单核系统中，内核也会在多个线程之间切换执行，导致指令流交错，从而可能引发类似问题。设备中断同样可能在任意时刻打断当前执行流，导致中断处理程序修改正在访问的共享数据。为了应对这些并发问题，操作系统设计采用了多种同步机制，如锁，以确保多个执行流在访问共享资源时不会发生冲突。

锁提供了互斥，确保在任何时刻只有一个 CPU 可以持有锁。当程序员将每个共享数据项与一个锁关联，并确保在访问数据时始终持有相应的锁时，数据项在同一时刻只能被一个 CPU 使用。在这种情况下，我们称锁为“保护”数据项。尽管锁是一个易于理解的并发控制机制，它的缺点在于可能会显著降低性能，因为锁会串行化本应并行的操作。

竞争条件指的是一个内存位置被并发访问，且至少有一个访问是写入操作。竞争通常是 bug 的表现，可能导致更新丢失或读取到不完整的数据。竞争的结果取决于两个 CPU 执行的实际顺序，以及内存系统如何排序这些操作，这使得竞争引发的 bug 难以复现和调试。例如，插入 `printf` 语句进行调试时，可能会改变执行时间，从而使竞争条件消失。

当我们说锁保护数据时，实际意味着锁保护了应用在数据上的一系列不变性。一个操作的正确性依赖于操作开始时不变性是否成立。虽然操作可能会暂时违反不变性，但必须在操作结束前恢复不变性。例如，对于链表，其不变性是头指针指向第一个元素，且每个元素的 `next` 域指向下一个元素。执行 `push` 操作时，`l->next = list` 会暂时破坏不变性，因为头指针不再指向第一个元素。竞争条件发生时，另一个 CPU 上的操作依赖于这些不变性，而这些不变性被暂时破坏了。使用锁可以保证在数据结构的临界区内只有一个 CPU 执行操作，从而避免在不变性被破坏时执行操作。

可以认为，锁通过将并发的临界区串行化，确保在任何时刻只有一个操作在执行，从而保护了不变性。也可以将被锁保护的临界区视为对其他临界区具有原子性，确保每个临界区的修改对其他操作来说是完整的，永远不会出现部分修改的情况。

正如之前所说，尽管锁的正确使用能够确保代码的正确性，但它也会影响性能。例如，当两个进程同时调用 `kfree` 时，锁会将这两个调用串行化，这意味着即使它们在不同的 CPU 上运行，也无法获得并行的性能收益。因此在内核设计中，一个重要的课题是如何减少锁争用。虽然 XV6 在这方面的处理较为简单，但更复杂的内核会通过优化数据结构和算法来减少锁争用。例如，内核可能会为每个 CPU 维护独立的空闲内存链表，只有在当前 CPU 的链表为空时，才会向其他 CPU 请求空闲内存。其他形式的争用可能需要更为复杂的设计。

此外，锁的位置也会显著影响性能。例如，在 `push` 操作中，如果将 `acquire` 放在较前的位置，会确保更早地获取锁，但这也会使 `malloc` 调用被串行化，从而降低性能。

## 自旋锁
XV6中有两种锁：自旋锁和睡眠锁。自旋锁定义为 `struct spinlock`，最重要的域就是 `locked`，1 代表被持有而 0 代表未被持有。理论上 XV6 可以通过下列代码来上锁：

```c
void
acquire(struct spinlock *lk) // does not work!
{
  for(;;) {
    if(lk->locked == 0) {
      lk->locked = 1;
      break;
    }
  }
}
```

但这种方式无法在多个处理器之间实现真正的互斥。例如，当两个 CPU 同时读取 `locked` 并看到其值为 0 时，它们可能都会尝试获取该锁，从而导致同时进入临界区，无法保证互斥性。因此，我们需要将这些操作原子化。

为此，多核处理器通常提供专门的原子指令，并依赖特殊的硬件机制来保障其正确性。在 RISC-V 架构中，这条原子交换指令是 `amoswap r, a`。它会将寄存器 `r` 中的值写入内存地址 `a`，同时将该地址原有的值读出并存入寄存器 `r`，实现寄存器与内存之间内容的交换。整个过程由底层硬件以原子方式完成，确保在读写之间没有其他 CPU 能访问该内存地址。这种特殊的硬件支持是实现互斥的关键，使得操作系统能够在多处理器环境中安全地管理共享资源。

获取锁的过程通过 `acquire` 函数实现，它调用可移植的C库函数 `__sync_lock_test_and_set`，其在底层就是通过 `amoswap` 来实现的。该函数的返回值是 `locked` 原先的值，也就是交换前的内容。

`acquire` 在一个循环中不断调用这个交换操作，直到成功获取锁为止（这被称为**自旋**）。每次循环中，函数将值 1 写入 `locked`，并通过原子交换得到它之前的值：如果旧值为 0，说明锁原本未被占用，当前线程成功获取了锁，同时也已将 `locked` 设置为 1；如果旧值为 1，则表示锁已被其他 CPU 持有，虽然进行了交换，但 `locked` 的值仍然是 1，锁未被获取，循环将继续尝试。

获取锁后，用于调试，`acquire` 将记录下来获取锁的 CPU。`lk->cpu` 字段受锁保护，只能在保持锁时更改。

`release` 函数则与 `acquire` 相反，该函数需要清空 CPU 域并释放锁。理论上释放只需要将 `locked` 域置 0 即可。但因为C语言标准运行编译器使用多存储指令来实现赋值，因此一条赋值语句也可能不是原子的。因此，`release` 需要使用C库函数 `__sync_lock_release` 来进行原子性赋值，它的底层也是通过 `amoswap` 指令实现的。

我们这里的底层之所以不使用 `load` 或者 `store` 来读写，是因为它们尽管看起来很简单，但并不是原子的，实际实现中可能由几个小步骤来实现。

## 锁的使用
我们往往不得不使用锁，但锁的使用又会降低效率，所以使用多少锁和他们各自保护哪些不变量都值得仔细斟酌。最基本的原则是：

- 当一个变量被一个 CPU 写入时，如果其他 CPU 也可以对其进行读写，就需要使用锁来避免操作重叠
- 如果一个不变量涉及多个内存位置，那么每个位置都需要被单独的锁来保护，从而保证不变量

如果不考虑效率，我们完全可以通过限制同一时刻只允许一个线程执行来避免所有并发问题。一个简单的做法是在多处理器系统上为整个内核设置一个锁：每当线程进入内核时，必须先获取这个锁，在离开内核时再释放它（尽管如管道读取或 `wait` 的系统调用会带来问题）。这样可以确保在任意时刻，只有一个线程在执行内核代码，从而完全避免并发带来的风险。很多单处理器系统已经使用这种方法来在多处理器上运行，这被称作“大内核锁”。但这破坏了并行性，为了提高效率，我们往往倾向于使用一组更细粒度的锁来让内核可以同时在多个处理器上执行。

一个粗粒度锁的例子是 XV6 的 `kalloc.c` 的分配器，它只有一个被锁保护的空闲链表。如果多个进程申请页面，他们都需要在 `acquire` 中自旋等待。为了提升效率，可以改变分配器的设计，使用多个空闲链表，让每个链表单独持有锁，从而允许真正的并行分配来提高性能。

一个细粒度锁的例子是 XV6 的文件锁，XV6 对于每个文件都有一个单独的锁，操作不同文件的进程无需等待其他的文件的锁。文件锁模式的粒度可以变得更加的细，比如控制每个区域之类的。总之，粒度需要综合锁的复杂性与性能度量来决定。

XV6中使用的锁如下表所示：

| 锁名            | 描述                                                         |
|-----------------|--------------------------------------------------------------|
| `bcache.lock`   | 保护块缓冲区缓存项的分配                                     |
| `cons.lock`     | 串行化对控制台硬件的访问，避免混合输出                       |
| `ftable.lock`   | 串行化文件表中文件结构的分配                                 |
| `icache.lock`   | 保护索引结点缓存项的分配                                     |
| `vdisk_lock`    | 串行化对磁盘硬件和 DMA 描述符队列的访问                      |
| `kmem.lock`     | 串行化内存分配                                               |
| `log.lock`      | 串行化事务日志操作                                           |
| `pi->lock`      | 串行化每个管道的操作                                         |
| `pid_lock`      | 串行化 `next_pid` 的递增                                     |
| `p->lock`       | 串行化进程状态的改变                                         |
| `tickslock`     | 串行化时钟计数操作                                           |
| `ip->lock`      | 串行化索引结点及其内容的操作                                 |
| `b->lock`       | 串行化每个块缓冲区的操作                                     |

## 死锁和锁排序
如果某段内核代码在执行过程中必须同时持有多个锁，那么所有相关的代码路径必须按照相同的顺序依次获取这些锁。这种统一的获取顺序对于避免死锁至关重要。若不同路径以不同顺序获取锁，就可能出现循环等待的情况，从而导致循环等待死锁。所以我们需要约定一个获取锁的顺序。

XV6 中存在许多长度为 2 的锁顺序链，通常涉及进程锁，其中一个常见的场景是 `sleep` 函数的使用。例如，`consoleintr` 是用于处理输入字符的中断处理函数。当新的一行输入到达时，内核会唤醒所有等待控制台输入的进程。在调用 `wakeup` 时，`consoleintr` 持有 `cons.lock`，而 `wakeup` 会获取目标进程的锁来修改其状态。为了避免死锁，必须遵循一条锁顺序规则：`cons.lock` 必须在任意进程锁之前获取。

XV6 文件系统代码中包含了最长的锁链。例如，在创建一个新文件时，系统可能需要依次获取：目录的锁、新文件的 inode 锁、磁盘块缓冲区的锁、磁盘驱动的 `vdisk_lock`，以及调用进程的锁。为了避免死锁，文件系统代码通常严格按照约定的顺序获取这些锁。

然而，遵守全局锁顺序以避免死锁可能非常困难。首先，锁顺序有时会与程序结构的逻辑冲突，例如模块 M1 调用了模块 M2，那么锁顺序要求必须先知道 M2 中所有的锁，但我们模块化的目的本就是希望将不同的功能模块分离开，使得每个模块在功能上独立。其次，有时在获取一个锁之前，并不能预知接下来需要获取哪个锁。比如，在文件系统中根据路径名逐级查找目录项时，只有在拿到当前目录的锁后，才能确定下一个要访问的目录或文件，从而决定接下来的锁。此外，`wait` 和 `exit` 函数在遍历进程表查找子进程时也存在类似情况。

为了降低循环等待死锁的风险，锁的设计往往不得不牺牲一定的并发性，采用较粗粒度的策略。锁的数量越多、粒度越细，系统中可能出现的锁依赖关系就越复杂，死锁的可能性也随之上升。因此，在内核实现中，如何有效地避免死锁是一项至关重要的设计任务。

在自旋锁和中断的交互中也可能有死锁风险。在 XV6 中，一些自旋锁用于保护既可能被内核线程访问，又可能被中断处理程序访问的共享数据。例如，定时器中断处理程序 `clockintr` 会更新 `ticks` 变量，而与此同时，一个内核线程可能正在 `sys_sleep` 中读取 `ticks`。`tickslock` 会将这两种访问串行化，确保在任意时刻只有一个执行流可以访问 `ticks`。

因此，自旋锁与中断的交互可能引发严重问题。假设 `sys_sleep` 获取了 `tickslock` 后运行时被定时器中断打断，中断处理程序 `clockintr` 随即尝试获取同一个锁。由于锁已被持有，`clockintr` 会忙等等待锁释放。然而，只有 `sys_sleep` 能释放该锁，而它必须等中断返回才能继续执行。这就造成了死锁：CPU 被困在中断处理程序中，无法继续运行原线程，也就无法释放锁，进而导致所有依赖该锁的代码陷入冻结状态。

为了避免这种情况，如果某个自旋锁会在中断处理程序中被使用，那么必须保证在中断开启的情况下，CPU 不会持有该锁。XV6 采取了更为保守的策略：**每当 CPU 申请任何自旋锁时，都会在本地关闭中断**。这样可以确保当前 CPU 不会在持锁期间被中断打断，从而避免自陷式死锁。需要注意的是，中断在其他 CPU 上仍然是可以发生的，因此这些中断处理程序依然可以尝试获取锁并等待，只要等待的不是当前 CPU 持有的锁，就不会造成问题。

当一个 CPU 不再持有自旋锁时，中断会被重新允许。为了处理嵌套的临界区，XV6 通过一些小的记录来跟踪当前 CPU 的嵌套层次。具体来说，`acquire` 调用 `push_off`，而 `release` 调用 `pop_off` 来管理临界区的嵌套状态。当计数器为 0 时，`pop_off` 会恢复到最外层临界区开始前的中断允许状态。`intr_off` 和 `intr_on` 分别用于执行 RISC-V 指令来关闭和开启中断，同时记录当前的中断状态，`intr_get` 则用于获取当前的中断允许状态。

这里还有一个需要注意的小细节：在 `acquire` 设置 `lk->locked` 之前调用 `push_off` 是非常重要的。如果两者顺序交换，就会存在一个小窗口：此时锁已经被获取，但中断却仍然是允许的。如果在这个窗口期间发生了中断，可能会导致系统死锁。类似地，在 `release` 释放锁之后再调用 `pop_off` 也非常关键，确保在释放锁后恢复正确的中断状态，以避免潜在的死锁或中断问题。

## 指令和内存访问排序
人们通常认为程序是按照源代码中语句的顺序执行的。然而，为了提高性能，许多编译器和 CPU 并不按照源代码的顺序执行指令。事实上，编译器和 CPU 往往会采用指令重排技术来提高执行效率。

当一条指令需要较长时间才能完成时，CPU 可能会提前发出其他指令，以便与当前指令的执行重叠，从而避免 CPU 停顿。例如，在顺序执行的指令A和B之间，如果它们之间没有依赖关系，CPU 可能会选择先执行指令B。这样做的原因是，指令B可能先准备好所需的输入，或者 CPU 可以通过同时执行A和B来提高吞吐量。

编译器也会使用类似的技术，称为“指令重排”，它通过在源代码中先发出一条语句的指令，然后再发出另一条语句的指令，从而优化执行顺序。这种优化不仅提升了性能，还避免了 CPU 空闲等待的情况。

编译器和 CPU 在进行指令重排时需要遵循一定规则，以确保不会改变正确编写的串行代码的执行结果。然而，这些规则确实允许在重排后改变并发代码的执行结果，并且这种改变很容易在多处理器系统上引发不正确的行为。我们将 CPU 的这种排序规则称为内存模型。

例如，在 `push` 的代码中，编译器或 CPU 可能会将对应于第4行的存储指令移动到第6行 `release` 后的某个地方，这会导致并发的风险：
```c
l = malloc(sizeof *l);
l->data = data;
acquire(&listlock);
l->next = list;
list = l;
release(&listlock);
```

如果发生这样的重新排序，就会出现一个窗口期，在此期间另一个 CPU 可能会获取锁并访问已经更新的 `list`，但此时 `list->next` 可能尚未初始化，导致读取到未定义或错误的值。

为了防止硬件和编译器执行此类重新排序，XV6 在 `acquire` 和 `release` 函数中都使用了 `__sync_synchronize()`。该函数是一种内存屏障，用于指示编译器和 CPU 不得跨越该屏障对 load 或 store 指令进行重排。由于 XV6 在访问共享数据时通过加锁来实现同步，这些内存屏障在几乎所有重要场景下都能确保内存操作的顺序性——尽管在后续的课程中可能会涉及一些例外情况。

## 睡眠锁
自旋锁类似于轮询，而睡眠锁则更像中断机制。

有时，XV6 需要长时间持有某个锁。例如，文件系统在对磁盘进行读写时会持有文件锁，而这些操作可能耗时数十毫秒。当其他进程尝试获取该锁时，如果使用的是自旋锁，就会造成大量浪费，因为等待锁的进程会持续占用 CPU 进行忙等。

自旋锁还有一个缺点：持锁进程不会让出（yield）CPU。但当它在等待磁盘等慢速资源时，我们希望其他进程能够使用 CPU。然而正如之前所说，持有自旋锁时让出 CPU 是不安全的，因为这可能导致死锁。例如，第二个线程在尝试获取该锁时持续自旋，可能会阻碍第一个线程运行，从而无法释放锁。

因此，XV6 引入了一种新的锁机制：在等待获取锁时，它可以让出 CPU；即使在已经持有锁的情况下，也允许让出 CPU 并重新开启中断。

XV6 以睡眠锁（sleep-locks）的形式提供了这种锁。`acquiresleep` 函数利用下一章将介绍的机制实现了这一点。从结构上看，睡眠锁内部有一个 `locked` 字段，并使用一个自旋锁来保护它。`acquiresleep` 会调用 `sleep`，在原子地释放自旋锁的同时让出 CPU，这样其他线程就可以在它等待期间继续执行。

由于睡眠锁在持锁期间保持中断开启，因此不能在中断处理程序中使用。同时，由于 `acquiresleep` 可能会让出 CPU，睡眠锁也不能在自旋锁的临界区中使用（尽管可以在睡眠锁的临界区中使用自旋锁）。

考虑到自旋锁在等待期间会浪费 CPU 时间，它更适用于临界区较短的场景；而睡眠锁则更适合用于执行时间较长的操作。

# 实验任务
## Memory allocator(moderate)
正如我们之前所说，XV6 的内存分配器使用一个全局锁来保护空闲链表，这样虽然简单，但会导致性能下降。我们将通过实现一个更细粒度的锁来提高性能。

我们将为每个 CPU 分配一个独立的空闲链表，并为每个链表分配一个锁。这样，每个 CPU 都可以独立地从自己的链表中分配内存，而不需要等待其他 CPU 释放锁。在 CPU 自身的链表内存不足时，它会尝试从其他 CPU 的链表中获取内存。

先修改kmem结构体，变成 NCPU 个链表。
```c
struct {
  struct spinlock lock;
  struct run *freelist;
} kmem[NCPU];
```

然后修改 `kinit` 函数，初始化每个锁。
```c
void
kinit()
{
  for(int i = 0; i < NCPU; i++)
    initlock(&kmem[i].lock, "kmem");
  freerange(end, (void*)PHYSTOP);
}
```

然后修改 `kfree` 函数，使用 `cpuid()` 来获取当前 CPU 的id，注意要禁用中断。
```c
void
kfree(void *pa)
{
  struct run *r;

  if(((uint64)pa % PGSIZE) != 0 || (char*)pa < end || (uint64)pa >= PHYSTOP)
    panic("kfree");

  // Fill with junk to catch dangling refs.
  memset(pa, 1, PGSIZE);

  r = (struct run*)pa;

  push_off();
  int cpu_id = cpuid();
  acquire(&kmem[cpu_id].lock);
  r->next = kmem[cpu_id].freelist;
  kmem[cpu_id].freelist = r;
  release(&kmem[cpu_id].lock);
  pop_off();
}
```

最后修改 `kalloc` 函数，尝试从当前 CPU 的链表中分配内存，如果当前链表为空，则尝试从其他 CPU 的链表中获取内存。
```c
void *
kalloc(void)
{
  struct run *r;

  push_off();
  int cpu_id = cpuid();
  acquire(&kmem[cpu_id].lock);
  r = kmem[cpu_id].freelist;
  if(r)
    kmem[cpu_id].freelist = r->next;
  release(&kmem[cpu_id].lock);
  if(r == 0)
  {
    for(int i = 0; i < NCPU; i++)
    {
      int next_cpu_id = (cpu_id + i + 1) % NCPU;
      acquire(&kmem[next_cpu_id].lock);
      r = kmem[next_cpu_id].freelist;
      if(r)
        kmem[next_cpu_id].freelist = r->next;
      release(&kmem[next_cpu_id].lock);
      if(r)
        break;
    }
  }

  if(r)
    memset((char*)r, 5, PGSIZE); // fill with junk
  pop_off();
  return (void*)r;
}
```

这样就完成了内存分配器的修改。现在每个 CPU 都有自己的空闲链表和锁，可以独立地进行内存分配和释放。当然，为了区分每个锁，我们还可以对锁用 `snprintf` 进行格式化命名。

## Buffer cache(hard)
呃，怎么说呢，虽然还没学缓冲区，但这个实验的确不需要知道什么东西。我们的目的还是细化锁的粒度，因此我们会考虑把缓冲区拆成多个部分，用哈希表来选择从哪个缓冲区中分配。然后，拆开后把所有锁给无脑改掉就行了……
```c
#define NBUCKET 13
#define NBUCKETSIZE 10

struct {
  struct spinlock lock;
  struct buf buf[NBUCKETSIZE]; // Buffer structures
  struct buf head;
} bcache[NBUCKET];
```

hash函数就用简单的取模即可，复杂的反而可能引入更多的竞争。
```c
#define HASH(dev, blockno) blockno % NBUCKET
```

然后在 `binit` 中初始化锁。
```c
void
binit(void)
{
  struct buf *b;

  for(int i = 0; i < NBUCKET; i++)
  {
    initlock(&bcache[i].lock, "bcache");
    bcache[i].head.prev = &bcache[i].head; // double linked list
    bcache[i].head.next = &bcache[i].head;
    for(b = bcache[i].buf; b < bcache[i].buf + NBUCKETSIZE; b++)
    {
      b->next = bcache[i].head.next;
      b->prev = &bcache[i].head;
      initsleeplock(&b->lock, "buffer");
      bcache[i].head.next->prev = b;
      bcache[i].head.next = b;
    }
  }
}
```
在 `bget` 中获取锁。
```c
static struct buf*
bget(uint dev, uint blockno)
{
  struct buf *b;

  int hash = HASH(dev, blockno);
  acquire(&bcache[hash].lock);

  // Is the block already cached?
  for(b = bcache[hash].head.next; b != &bcache[hash].head; b = b->next){
    if(b->dev == dev && b->blockno == blockno){
      b->refcnt++;
      release(&bcache[hash].lock);
      acquiresleep(&b->lock);
      return b;
    }
  }

  // Not cached.
  // Recycle the least recently used (LRU) unused buffer.
  for(b = bcache[hash].head.prev; b != &bcache[hash].head; b = b->prev){
    if(b->refcnt == 0) {
      b->dev = dev;
      b->blockno = blockno;
      b->valid = 0;
      b->refcnt = 1;
      release(&bcache[hash].lock);
      acquiresleep(&b->lock);
      return b;
    }
  }
  panic("bget: no buffers");
}
```

然后`brelse`,`bpin` 和 `bunpin` 也一样。
```c
void
brelse(struct buf *b)
{
  if(!holdingsleep(&b->lock))
    panic("brelse");

  releasesleep(&b->lock);

  int hash = HASH(b->dev, b->blockno);
  acquire(&bcache[hash].lock);  
  b->refcnt--;
  if (b->refcnt == 0) {
    // no one is waiting for it.
    b->next->prev = b->prev;
    b->prev->next = b->next;
    b->next = bcache[hash].head.next;
    b->prev = &bcache[hash].head;
    bcache[hash].head.next->prev = b;
    bcache[hash].head.next = b;
  }
  
  release(&bcache[hash].lock);
}

void
bpin(struct buf *b) {
  int hash = HASH(b->dev, b->blockno);
  acquire(&bcache[hash].lock);
  b->refcnt++;
  release(&bcache[hash].lock);
}

void
bunpin(struct buf *b) {
  int hash = HASH(b->dev, b->blockno);
  acquire(&bcache[hash].lock);
  b->refcnt--;
  release(&bcache[hash].lock);
}
```
然后就没了……真就是完全不要动脑子，直接拆开就行了。

# 小结
Lab8 也相当的简单，就是让你细化锁的粒度。但是吧，这种单独拆分完全没有啥技术含量啊，毕竟前人已经把大粒度的实现了，你要做的只是拆分，都不需要理解原理什么的……不过虽然实验简单，但在实际中锁的粒度却很难把控，尤其是当锁的数量增多时，可能会导致死锁等问题。我们在实验中只需要考虑简单的锁顺序和避免死锁，但在实际中可能会遇到更复杂的情况，需要更深入的思考和设计。

无论如何，锁还是一个相当实用的工具，能够有效地解决并发问题。如果要自己设计，想必也会相当麻烦。

最后照例附上通关截图：

<img src="/illustrations/MIT-6-S081-Lab8/1.png" alt="通关截图">