---
title: UCAS-nlp-Week3：词向量
date: 2025-10-14 15:28:35
categories: UCAS-nlp
description: 词向量的基本概念和实现。
tags: [自然语言处理, 词向量]
---
# 词向量
## 文本表示
文本由文字和标点构成，是一种字符串形式的信息载体。它可以按不同粒度划分为词、短语、句子、段落乃至篇章。要让计算机“理解”文本，首先必须将其转化为一种**形式化的数值表示**，以便反映内容特征并区分不同文本。最简单的方式是 **One-hot（独热）表示**。

- **One-hot 表示** 是一种基础的文本向量化方法：为每个词（或字符）分配一个唯一的索引，并构建一个与词汇表等长的向量——在该词对应的索引位置取值为 1，其余位置为 0。

不难看出， One-hot 表示存在以下两个主要问题：

1. **维度灾难**  
   随着词汇表规模增大，向量维度会急剧上升，带来存储与计算的高开销。同时，这种表示极度稀疏（大部分元素为 0），计算效率低下。
2. **语义鸿沟**  
   One-hot 向量无法体现词与词之间的语义关系。任意两个不同词的向量都是正交的（欧氏距离恒为 √2），即使语义相近（如“猫”和“狗”），模型也无法感知它们的相似性。

相比于稀疏的 One-hot 表示，更加有效的做法是使用**低维实值向量表示**。  

这种方法将每个单词表示为一个 **D 维的稠密向量**（其中 D 远小于词汇表的大小）。向量中的每个维度不再只是占位符，而是用于刻画该词的某种语义或语法特征。通过学习得到的这些向量可以使语义相近的词在向量空间中彼此靠近，从而在一定程度上捕捉词语间的语义关联。

实现这种表示的常用模型之一是 **连续词袋模型（Continuous Bag-of-Words, CBOW）**。

### CBOW
CBOW 是一种典型的词向量学习方法。该模型忽略词序，只关注上下文中出现的词，通过利用上下文词语来预测目标（中心）词，从而在训练过程中逐渐学习出词语之间的语义关系。其整个训练过程可以概括为以下几个步骤：

1. 初始化词向量矩阵 L

    词汇表可以根据语料确定，可选取出现频率最高的 N 个单词，或频数超过某一阈值 K 的单词作为收录对象。词向量维度 D 是一个超参数，由实验者自行设定，通常远小于词汇表的规模。词向量矩阵 L 的形状如下：

    $$
    L \in \mathbb{R}^{|V| \times D}
    $$

    其中 $|V|$ 表示词汇表大小，每一行对应一个词的 D 维向量。

2. 计算上下文向量的均值向量 h

    对于一个目标中心词 $w\_i$，设它的上下文窗口大小为 C（即左边 C 个词和右边 C 个词），记上下文词为 
    $$w_{i-C}, \dots, w_{i-1}, w_{i+1}, \dots, w_{i+C}$$
    
    CBOW 模型将这些上下文词的向量取平均，得到上下文的语义表示向量 $h$：

    $$
    h = \frac{1}{2C} \sum_{k=i-C, k \ne i}^{i+C} e_{w_k}
    $$

    其中 $e_{w_k}$ 表示词 $w_k$ 的词向量。

3. 使用 Softmax 函数计算中心词的概率

    CBOW 使用 Softmax 层来计算在给定上下文向量 $h$ 的情况下，每个词作为中心词的概率。对于目标词 $w_i$：

    $$
    P(w_i|WC) = \frac{\exp(h \cdot e_{w_i})}{\sum_{k=1}^{|V|} \exp(h \cdot e_{w_k})}
    $$

    其中分子表示目标词与上下文的相似度，分母为对整个词表的归一化， $WC$ 是上下文窗口内的所有词。

4. 最大化目标函数

    模型通过最大化整个语料中所有目标词的条件概率来训练：

    $$
    L = \sum_{w_i \in D} \log P(w_i | WC)
    $$

    训练完成后，词向量矩阵中的每一行即为学习得到的词向量，它能在语义空间中反映词语间的相似关系。
